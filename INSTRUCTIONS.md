# Usage Instructions

This document provides instructions on how to set up and use the Wet Bulb Temperature Analysis project.

## Setup

### Option 1: Using pip

1. Create a virtual environment (optional but recommended):
   ```
   python -m venv venv
   ```

2. Activate the virtual environment:
   - On Windows:
   ```
   venv\Scripts\activate
   ```
   - On macOS/Linux:
   ```
   source venv/bin/activate
   ```

3. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

### Option 2: Using conda

1. Create and activate a conda environment:
   ```
   conda env create -f environment.yaml
   conda activate wet-bulb-temp
   ```

## Verifying Environment Setup

To ensure your environment is properly configured:

```
python scripts/verify_environment.py
```

This will check that all required libraries are installed and accessible.

## Processing Raw Data

To clean and prepare the raw data files for analysis:

```
python scripts/preprocess_data.py
```

This script will:
- Load raw climate and greenhouse gas data
- Clean, transform, and combine the datasets
- Save the processed data for analysis and visualization

## Running the Streamlit Dashboard

The Streamlit dashboard provides an interactive way to explore the data and visualize the relationships between wet-bulb temperature and various climate variables.

1. Run the dashboard using the convenience script:
   ```
   python run_dashboard.py
   ```

   Or run it directly with Streamlit:
   ```
   streamlit run dashboard/app.py
   ```

2. Once the dashboard is running, you can access it in your web browser at http://localhost:8501

3. Use the navigation menu in the sidebar to explore different aspects of the data:
   - Home: Overview with key metrics
   - Data Explorer: Examine variable distributions
   - Time Series Analysis: Trends and patterns over time
   - Correlation Analysis: Relationships between variables
   - Regression Modeling: Build predictive models
   - About: Project information and methodology

## Running Sample Analysis

To run a quick analysis that demonstrates the key features of the project:

```
python scripts/analyze.py
```

This will generate visualizations and regression analysis results in the `data/output` directory.

## Working with Notebooks

The project includes two main notebooks:

1. **Original Analysis Notebook** (`notebooks/data_analysis_of_wet_bulb_temperature.ipynb`):
   - Comprehensive exploratory analysis with detailed explanations
   - Includes background research and in-depth interpretation

2. **Sample Analysis Notebook** (`notebooks/sample_analysis.ipynb`):
   - Streamlined version demonstrating key analysis techniques
   - Can be regenerated using:
     ```
     python scripts/create_sample_notebook.py
     ```

## Using the Python Modules

If you want to use the Python modules directly in your own scripts or notebooks, you can follow these steps:

1. Import the required modules:
   ```python
   from src.data_processing.data_loader import load_data, prepare_data_for_analysis
   from src.visualization.exploratory import plot_time_series, plot_correlation_matrix
   from src.models.regression import build_linear_regression_model, evaluate_regression_model
   ```

2. Load and prepare the data:
   ```python
   data_folder = "path/to/data"
   data = prepare_data_for_analysis(data_folder)
   ```

3. Visualize the data:
   ```python
   import matplotlib.pyplot as plt
   fig = plot_time_series(data, 'avg_wet_bulb', rolling_window=12)
   plt.show()
   ```

4. Build and evaluate a regression model:
   ```python
   from src.models.regression import preprocess_for_regression
   
   target = 'avg_wet_bulb'
   features = ['mean_air_temp', 'mean_relative_humidity', 'average_co2']
   
   X_train, X_test, y_train, y_test, scaler = preprocess_for_regression(
       data, target, features
   )
   
   model = build_linear_regression_model(X_train, y_train)
   results = evaluate_regression_model(model, X_train, X_test, y_train, y_test, features)
   ```

## Project Structure Overview

The project is organized into the following key components:

```
Data-Analysis-of-Wet-Bulb-Temperature/
â”œâ”€â”€ ğŸ›ï¸ dashboard/           # Streamlit interactive dashboard
â”‚   â”œâ”€â”€ app.py              # Main dashboard application (189 lines)
â”‚   â””â”€â”€ __init__.py         # Package initialization
â”œâ”€â”€ ğŸ“Š data/                # Data storage and outputs
â”‚   â”œâ”€â”€ raw/                # Original datasets (11 files)
â”‚   â”‚   â”œâ”€â”€ wet-bulb-temperature-hourly.csv      # 365K+ hourly records
â”‚   â”‚   â”œâ”€â”€ surface-air-temperature-monthly-mean.csv
â”‚   â”‚   â”œâ”€â”€ M890081.csv     # Singapore climate variables
â”‚   â”‚   â”œâ”€â”€ co2_mm_mlo.csv  # Global COâ‚‚ concentrations
â”‚   â”‚   â”œâ”€â”€ ch4_mm_gl.csv   # Global CHâ‚„ concentrations
â”‚   â”‚   â”œâ”€â”€ n2o_mm_gl.csv   # Global Nâ‚‚O concentrations
â”‚   â”‚   â”œâ”€â”€ sf6_mm_gl.csv   # Global SFâ‚† concentrations
â”‚   â”‚   â””â”€â”€ ...             # Additional datasets
â”‚   â”œâ”€â”€ processed/          # Clean, analysis-ready data
â”‚   â”‚   â”œâ”€â”€ final_dataset.csv       # Merged analysis dataset (497 records)
â”‚   â”‚   â””â”€â”€ dataset_description.md  # Data documentation
â”‚   â””â”€â”€ output/             # Generated visualizations
â”‚       â”œâ”€â”€ correlation_matrix.png
â”‚       â”œâ”€â”€ feature_importance.png
â”‚       â”œâ”€â”€ temp_scatter.png
â”‚       â””â”€â”€ wet_bulb_time_series.png
â”œâ”€â”€ ğŸ““ notebooks/           # Jupyter analysis notebooks
â”‚   â”œâ”€â”€ data_analysis_of_wet_bulb_temperature.ipynb  # Original research (1,502 lines)
â”‚   â”œâ”€â”€ sample_analysis.ipynb                        # Usage demonstration
â”‚   â””â”€â”€ project_evolution.ipynb                      # Evolution analysis
â”œâ”€â”€ ğŸ› ï¸ scripts/            # Automation and utility scripts
â”‚   â”œâ”€â”€ analyze.py          # Complete analysis pipeline
â”‚   â”œâ”€â”€ preprocess_data.py  # Data cleaning and merging
â”‚   â”œâ”€â”€ create_sample_notebook.py   # Generate example notebooks
â”‚   â””â”€â”€ verify_environment.py       # Environment validation
â”œâ”€â”€ ğŸ§© src/                # Core Python modules (25+ files)
â”‚   â”œâ”€â”€ app_pages/          # Modular dashboard pages (6 components)
â”‚   â”‚   â”œâ”€â”€ home.py         # Landing page with overview
â”‚   â”‚   â”œâ”€â”€ data_explorer.py        # Interactive data examination
â”‚   â”‚   â”œâ”€â”€ time_series.py  # Temporal analysis tools
â”‚   â”‚   â”œâ”€â”€ correlation.py  # Statistical relationships
â”‚   â”‚   â”œâ”€â”€ regression.py   # ML modeling interface
â”‚   â”‚   â””â”€â”€ about.py        # Project information
â”‚   â”œâ”€â”€ data_processing/    # Data loading and preprocessing
â”‚   â”‚   â””â”€â”€ data_loader.py  # Multi-source data integration (511 lines)
â”‚   â”œâ”€â”€ features/           # Feature engineering
â”‚   â”‚   â””â”€â”€ feature_engineering.py  # Temporal and derived features
â”‚   â”œâ”€â”€ models/             # Machine learning models
â”‚   â”‚   â””â”€â”€ regression.py   # Linear regression implementation
â”‚   â”œâ”€â”€ utils/              # Custom statistical functions
â”‚   â”‚   â””â”€â”€ statistics.py   # Manual statistical calculations
â”‚   â””â”€â”€ visualization/      # Plotting and visualization
â”‚       â””â”€â”€ exploratory.py  # Standardized visualizations (310 lines)
â”œâ”€â”€ ğŸ“‹ logs/                # Application logs
â”‚   â””â”€â”€ preprocessing.log   # Data processing logs
â”œâ”€â”€ ğŸ“„ Configuration Files
â”‚   â”œâ”€â”€ INSTRUCTIONS.md     # This usage guide
â”‚   â”œâ”€â”€ README.md           # Comprehensive project documentation (627 lines)
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies (pip)
â”‚   â”œâ”€â”€ environment.yaml    # Conda environment specification
â”‚   â”œâ”€â”€ run_dashboard.py    # One-command launcher
â”‚   â”œâ”€â”€ audit_report.md     # Code quality assessment
â”‚   â””â”€â”€ documentation_improvements.md  # Enhancement tracking
```

**Total Statistics:**
- **Python Files**: 25+ modules across 6 subsystems
- **Lines of Code**: 4,000+ lines (fully documented)
- **Data Coverage**: 1982-2023 (40+ years of climate data)
- **Records**: 497 monthly observations from 7 data sources

For a more detailed overview of the project structure and implementation details, please refer to the README.md file.
